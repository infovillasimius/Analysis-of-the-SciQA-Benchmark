{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Lsek5luwJW6ZrzrH6G6jcGAAG2QD7gVk","timestamp":1738860796555}],"gpuType":"T4","authorship_tag":"ABX9TyOMoiXJycq+TcQDdpM5bono"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"CCRq1Qr8MBk8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738947113871,"user_tz":-60,"elapsed":104086,"user":{"displayName":"Antonello Meloni","userId":"09336063038722863523"}},"outputId":"ad6e28ba-5c97-462a-bf40-91e774834a26"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"source":["!pip -q install transformers\n","!pip -q install accelerate>=0.12.0\n","!pip install datasets"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd drive/MyDrive/SCIQA"],"metadata":{"id":"kmV-ivVrGinc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738947245640,"user_tz":-60,"elapsed":48287,"user":{"displayName":"Antonello Meloni","userId":"09336063038722863523"}},"outputId":"b58fa1a6-5032-4ea3-cae3-877d0be602a0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/SCIQA\n"]}]},{"cell_type":"code","source":["import json\n","from tqdm import tqdm\n","\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSeq2SeqLM\n","from datasets import load_dataset\n","import torch\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","def clean(st):\n","    st = st.replace(\"\\n\", \" \")\n","    st = st.replace(\"?\", \" ?\")\n","    st = st.replace(\"{\", \" { \")\n","    st = st.replace(\"}\", \" } \")\n","    st = st.replace(\"\\\\'\", \"'\")\n","\n","    while \"  \" in st:\n","        st = st.replace(\"  \", \" \")\n","    return st.strip()\n","\n","\n","def get_entities(query):\n","    query = clean(query)\n","    entities = []\n","    relations = []\n","    all_good = []\n","    words = query.split(\" \")\n","    for word in words:\n","        if word.startswith(\"orkg\"):\n","            all_good.append(word)\n","\n","    for word in all_good:\n","        if word.startswith(\"orkgp:\"):\n","            relations.append(word)\n","        else:\n","            entities.append(word)\n","\n","    return {\"entities\": entities, \"relations\": relations}\n","\n","\n","prefix = \"translate English to Sparql: \"\n","tokenizer = AutoTokenizer.from_pretrained(\"sciqa_T5_model_we\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"sciqa_T5_model_we\").to(device)\n","\n","# books = load_dataset(\"json\", data_files={'test':'test.json'})\n","# books = load_dataset(\"orkg/SciQA\")\n","books = load_dataset(\"awalesushil/DBLP-QuAD\")\n","print(books[\"test\"])\n","\n","queries = []\n","sparql = []\n","\n","for feature in books[\"test\"]:\n","    # ents = get_entities(feature[\"query\"][\"sparql\"])\n","    # query = prefix + feature.get(\"question\").get(\"string\") + \"\\nentities: \" + str(ents.get(\"entities\")) + \"\\nrelations: \" + str(ents.get(\"relations\"))\n","    query = prefix + feature.get(\"question\").get(\"string\") + \"\\nentities: \" + str(feature.get(\"entities\")) + \"\\nrelations: \" + str(feature.get(\"relations\"))\n","    queries.append(query)\n","    gold_sparql = feature.get(\"query\").get(\"sparql\")\n","    sparql.append(gold_sparql)\n","\n","print(len(queries))\n","\n","def divide_chunks(l_, n_):\n","    # looping till length l\n","    for i_ in range(0, len(l_), n_):\n","        yield l_[i_:i_ + n_]\n","\n","n = 10\n","\n","q = list(divide_chunks(queries, n))\n","\n","gs = []\n","gst = []\n","i = 0\n","\n","for group in tqdm(q):\n","    # print(str(i)+\"%\", end=\"  \")\n","    # i += 0.5\n","    inputs = tokenizer(group, max_length=512, truncation=True, return_tensors='pt', padding=True).to(device)\n","    with torch.no_grad():\n","        generated_ids = model.generate(**inputs, max_new_tokens=512, do_sample=True, top_k=30, top_p=0.95)\n","\n","    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","    generated_texts2 = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)\n","\n","    generated_texts2 = [x.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip() for x in generated_texts2]\n","\n","    gs += generated_texts\n","    gst += generated_texts2\n","\n","result = {\"questions\": queries, \"sparql\": sparql, \"generated_sparql\": gs, \"generated_with_special_tokens\": gst}\n","\n","with open(\"SCIQA_we_ft_T5_results_DBLP_we.json\", \"w\", encoding=\"utf-8\") as text_file:\n","    print(json.dumps(result), file=text_file)"],"metadata":{"id":"9Dfz7Vz3Gb2E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738952463774,"user_tz":-60,"elapsed":2008547,"user":{"displayName":"Antonello Meloni","userId":"09336063038722863523"}},"outputId":"52faf309-0836-41c5-c5ab-188e0b2e3bd5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['id', 'query_type', 'question', 'paraphrased_question', 'query', 'template_id', 'entities', 'relations', 'temporal', 'held_out'],\n","    num_rows: 2000\n","})\n","2000\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [33:24<00:00, 10.02s/it]\n"]}]}]}